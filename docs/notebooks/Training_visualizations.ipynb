{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Visdom\n",
    "\n",
    "Install it with:\n",
    "\n",
    "`pip install visdom`\n",
    "\n",
    "Start the server:\n",
    "\n",
    "`python -m visdom.server`\n",
    "\n",
    "Visdom now can be accessed at http://localhost:8097 in the browser.\n",
    "\n",
    "\n",
    "# LDA Training Visualization\n",
    "\n",
    "To monitor the LDA training, a list of Metrics can be passed to the LDA function call for plotting their values live as the training progresses.  \n",
    "\n",
    "Let's plot the training stats for an LDA model being trained on Lee corpus. We will use the four evaluation metrics available for topic models in gensim: Coherence, Perplexity, Topic diff and Convergence. (using separate hold_out and test corpus for evaluating the perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import ldamodel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')\n",
    "\n",
    "def read_corpus(fname):\n",
    "    texts = []\n",
    "    with open(fname, encoding=\"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            # lower case all words\n",
    "            lowered = line.lower()\n",
    "            # remove punctuation and split into seperate words\n",
    "            words = re.compile('\\w+').findall(lowered)\n",
    "            texts.append(words)\n",
    "    return texts\n",
    "\n",
    "training_texts = read_corpus(lee_train_file)\n",
    "eval_texts = read_corpus(lee_test_file)\n",
    "\n",
    "# Split test data into hold_out and test corpus\n",
    "holdout_texts = eval_texts[:25]\n",
    "test_texts = eval_texts[25:]\n",
    "\n",
    "training_dictionary = Dictionary(training_texts)\n",
    "holdout_dictionary = Dictionary(holdout_texts)\n",
    "test_dictionary = Dictionary(test_texts)\n",
    "\n",
    "training_corpus = [training_dictionary.doc2bow(text) for text in training_texts]\n",
    "holdout_corpus = [holdout_dictionary.doc2bow(text) for text in holdout_texts]\n",
    "test_corpus = [test_dictionary.doc2bow(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n",
    "\n",
    "# define perplexity callback for hold_out and test corpus\n",
    "pl_holdout = PerplexityMetric(corpus=holdout_corpus, logger=\"visdom\", title=\"Perplexity (hold_out)\")\n",
    "pl_test = PerplexityMetric(corpus=test_corpus, logger=\"visdom\", title=\"Perplexity (test)\")\n",
    "\n",
    "# define other remaining metrics available\n",
    "ch_umass = CoherenceMetric(corpus=training_corpus, coherence=\"u_mass\", logger=\"visdom\", title=\"Coherence (u_mass)\")\n",
    "ch_cv = CoherenceMetric(corpus=training_corpus, texts=training_texts, coherence=\"c_v\", logger=\"visdom\", title=\"Coherence (c_v)\")\n",
    "diff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"visdom\", title=\"Diff (kullback_leibler)\")\n",
    "convergence_kl = ConvergenceMetric(distance=\"kullback_leibler\", logger=\"visdom\", title=\"Convergence (kullback_leibler)\")\n",
    "\n",
    "callbacks = [pl_holdout, pl_test, ch_umass, ch_cv, diff_kl, convergence_kl]\n",
    "\n",
    "# training LDA model\n",
    "model = ldamodel.LdaModel(corpus=training_corpus, id2word=training_dictionary, passes=3, num_topics=5, eval_every=None, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is set for training, you can open http://localhost:8097 to see the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.259766196856\n"
     ]
    }
   ],
   "source": [
    "# to get a metric value on a trained model\n",
    "print(CoherenceMetric(corpus=training_corpus, coherence=\"u_mass\").get_value(model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four types of graphs which are plotted for LDA:\n",
    "\n",
    "**Coherence**\n",
    "\n",
    "Coherence is a measure used to evaluate topic models. A good model will generate coherent topics, i.e., topics         with high topic coherence scores. Good topics are topics that can be described by a short label based on the topic     terms they spit out. \n",
    "\n",
    "<img src=\"Coherence.gif\">\n",
    "\n",
    "Now, this graph along with the others explained below, can be used to decide if it's time to stop the training. We     can see if the value stops changing after some epochs and that we are able to get the highest possible coherence       of our model.  \n",
    "\n",
    "\n",
    "**Perplexity**\n",
    "\n",
    "Perplexity is a measurement of how well a probability distribution or probability model predicts a sample. In LDA, topics are described by a probability distribution over vocabulary words. So, perplexity can be used to compare probabilistic models like LDA.\n",
    "\n",
    "<img src=\"Perplexity.gif\">\n",
    "\n",
    "For a good model, perplexity should be as low as possible.\n",
    "\n",
    "\n",
    "**Topic Difference**\n",
    "\n",
    "Topic Diff calculates the distance between two LDA models. This distance is calculated based on the topics, by either using their probability distribution over vocabulary words (kullback_leibler, hellinger) or by simply using the common vocabulary words between the topics from both model.\n",
    "\n",
    "<img src=\"Diff.gif\">\n",
    "\n",
    "In the heatmap, X-axis define the Epoch no. and Y-axis define the distance between the identical topic from consecutive epochs. For ex. a particular cell in the heatmap with values (x=3, y=5, z=0.4) represent the distance(=0.4) between the topic 5 from 3rd epoch and topic 5 from 2nd epoch. With increasing epochs, the distance between the identical topics should decrease.\n",
    "  \n",
    "  \n",
    "**Convergence**\n",
    "\n",
    "Convergence is the sum of the difference between all the identical topics from two consecutive epochs. It is basically the sum of column values in the heatmap above.\n",
    "\n",
    "<img src=\"Convergence.gif\">\n",
    "\n",
    "The model is said to be converged when the convergence value stops descending with increasing epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logs\n",
    "\n",
    "We can also log the metric values after every epoch to the shell apart from visualizing them in Visdom. The only change is to define `logger=\"shell\"` instead of `\"visdom\"` in the input callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.2\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.00013900472616068947\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (multi-pass) LDA training, 5 topics, 3 passes over the supplied corpus of 300 documents, updating model once every 300 documents, evaluating perplexity every 0 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #300/300\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.059*\"the\" + 0.031*\"in\" + 0.019*\"and\" + 0.018*\"a\" + 0.017*\"of\" + 0.016*\"to\" + 0.011*\"for\" + 0.008*\"is\" + 0.007*\"on\" + 0.007*\"s\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.056*\"the\" + 0.030*\"to\" + 0.023*\"a\" + 0.020*\"and\" + 0.016*\"in\" + 0.015*\"of\" + 0.009*\"said\" + 0.008*\"for\" + 0.008*\"is\" + 0.008*\"on\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.069*\"the\" + 0.031*\"of\" + 0.026*\"to\" + 0.018*\"a\" + 0.018*\"and\" + 0.017*\"in\" + 0.009*\"he\" + 0.009*\"s\" + 0.009*\"is\" + 0.009*\"for\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.058*\"the\" + 0.026*\"to\" + 0.023*\"of\" + 0.021*\"in\" + 0.016*\"and\" + 0.014*\"a\" + 0.011*\"he\" + 0.008*\"is\" + 0.007*\"are\" + 0.007*\"has\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.046*\"the\" + 0.022*\"to\" + 0.019*\"a\" + 0.018*\"in\" + 0.017*\"and\" + 0.017*\"of\" + 0.009*\"on\" + 0.009*\"s\" + 0.008*\"he\" + 0.007*\"for\"\n",
      "INFO:gensim.models.ldamodel:topic diff=1.998099, rho=1.000000\n",
      "INFO:gensim.models.ldamodel:Epoch 0: Perplexity estimate: 557760.366254\n",
      "INFO:gensim.models.ldamodel:Epoch 0: Perplexity estimate: 1398892.0238\n",
      "INFO:gensim.models.ldamodel:Epoch 0: Coherence estimate: -0.239114528508\n",
      "INFO:gensim.models.ldamodel:Epoch 0: Diff estimate: [ 0.87066886  0.89404796  1.          0.86438108  0.80259906]\n",
      "INFO:gensim.models.ldamodel:Epoch 0: Convergence estimate: 0.0\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 1, at document #300/300\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.056*\"the\" + 0.029*\"in\" + 0.018*\"and\" + 0.018*\"a\" + 0.017*\"to\" + 0.017*\"of\" + 0.011*\"for\" + 0.008*\"s\" + 0.008*\"he\" + 0.008*\"was\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.055*\"the\" + 0.029*\"to\" + 0.022*\"a\" + 0.019*\"and\" + 0.015*\"of\" + 0.015*\"in\" + 0.008*\"is\" + 0.008*\"for\" + 0.008*\"said\" + 0.007*\"on\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.070*\"the\" + 0.029*\"of\" + 0.026*\"to\" + 0.019*\"a\" + 0.018*\"and\" + 0.018*\"in\" + 0.009*\"he\" + 0.009*\"is\" + 0.009*\"s\" + 0.009*\"for\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.054*\"the\" + 0.025*\"to\" + 0.022*\"in\" + 0.020*\"of\" + 0.017*\"and\" + 0.014*\"a\" + 0.009*\"he\" + 0.008*\"is\" + 0.007*\"has\" + 0.007*\"mr\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.043*\"the\" + 0.021*\"to\" + 0.018*\"a\" + 0.017*\"in\" + 0.015*\"and\" + 0.015*\"of\" + 0.009*\"on\" + 0.009*\"he\" + 0.008*\"s\" + 0.008*\"for\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.639297, rho=0.577350\n",
      "INFO:gensim.models.ldamodel:Epoch 1: Perplexity estimate: 364255.929481\n",
      "INFO:gensim.models.ldamodel:Epoch 1: Perplexity estimate: 870027.350434\n",
      "INFO:gensim.models.ldamodel:Epoch 1: Coherence estimate: -0.263564259646\n",
      "INFO:gensim.models.ldamodel:Epoch 1: Diff estimate: [ 1.          0.65859849  0.27993545  0.86568637  0.70939543]\n",
      "INFO:gensim.models.ldamodel:Epoch 1: Convergence estimate: 0.0\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 2, at document #300/300\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.200): 0.055*\"the\" + 0.027*\"in\" + 0.018*\"to\" + 0.018*\"and\" + 0.018*\"a\" + 0.017*\"of\" + 0.011*\"for\" + 0.009*\"s\" + 0.008*\"he\" + 0.008*\"was\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.200): 0.055*\"the\" + 0.028*\"to\" + 0.021*\"a\" + 0.019*\"and\" + 0.016*\"of\" + 0.015*\"in\" + 0.009*\"is\" + 0.008*\"for\" + 0.008*\"said\" + 0.007*\"at\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.200): 0.070*\"the\" + 0.029*\"of\" + 0.026*\"to\" + 0.019*\"a\" + 0.019*\"in\" + 0.019*\"and\" + 0.010*\"is\" + 0.009*\"he\" + 0.009*\"s\" + 0.009*\"for\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.200): 0.053*\"the\" + 0.025*\"to\" + 0.023*\"in\" + 0.020*\"of\" + 0.018*\"and\" + 0.016*\"a\" + 0.009*\"palestinian\" + 0.008*\"he\" + 0.008*\"mr\" + 0.008*\"said\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.200): 0.041*\"the\" + 0.020*\"to\" + 0.017*\"a\" + 0.016*\"in\" + 0.014*\"and\" + 0.013*\"of\" + 0.009*\"he\" + 0.008*\"on\" + 0.008*\"for\" + 0.007*\"s\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.440013, rho=0.500000\n",
      "INFO:gensim.models.ldamodel:Epoch 2: Perplexity estimate: 307382.596558\n",
      "INFO:gensim.models.ldamodel:Epoch 2: Perplexity estimate: 724828.178637\n",
      "INFO:gensim.models.ldamodel:Epoch 2: Coherence estimate: -0.327008857571\n",
      "INFO:gensim.models.ldamodel:Epoch 2: Diff estimate: [ 0.68276338  0.59768978  0.20701571  1.          0.78838212]\n",
      "INFO:gensim.models.ldamodel:Epoch 2: Convergence estimate: 0.0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# define perplexity callback for hold_out and test corpus\n",
    "pl_holdout = PerplexityMetric(corpus=holdout_corpus, logger=\"shell\")\n",
    "pl_test = PerplexityMetric(corpus=test_corpus, logger=\"shell\")\n",
    "\n",
    "# define other remaining metrics available\n",
    "ch_umass = CoherenceMetric(corpus=training_corpus, coherence=\"u_mass\", logger=\"shell\")\n",
    "diff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"shell\")\n",
    "convergence_kl = ConvergenceMetric(distance=\"kullback_leibler\", logger=\"shell\")\n",
    "\n",
    "callbacks = [pl_holdout, pl_test, ch_umass, diff_kl, convergence_kl]\n",
    "\n",
    "# training LDA model\n",
    "model = ldamodel.LdaModel(corpus=training_corpus, id2word=training_dictionary, passes=3, num_topics=5, eval_every=None, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric values can also be accessed from the model instance for custom uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PerplexityMetric',\n",
       "  [557760.36625438754, 364255.92948058416, 307382.59655802854]),\n",
       " ('PerplexityMetric',\n",
       "  [1398892.0237993822, 870027.3504341701, 724828.17863728071]),\n",
       " ('CoherenceMetric',\n",
       "  [-0.23911452850779141, -0.26356425964553953, -0.32700885757127918]),\n",
       " ('DiffMetric',\n",
       "  [array([ 0.87066886,  0.89404796,  1.        ,  0.86438108,  0.80259906]),\n",
       "   array([ 1.        ,  0.65859849,  0.27993545,  0.86568637,  0.70939543]),\n",
       "   array([ 0.68276338,  0.59768978,  0.20701571,  1.        ,  0.78838212])]),\n",
       " ('ConvergenceMetric', [0.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
